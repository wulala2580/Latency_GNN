{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7545258d",
   "metadata": {},
   "source": [
    "# Latency Dataset - GNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc470fc",
   "metadata": {},
   "source": [
    "This example will demonstrate our ability to predict latency with data from NN-meter through GNN.\n",
    "\n",
    "Let's start our journey!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8466897",
   "metadata": {},
   "source": [
    "## Step 1: Download data and Construct our graph\n",
    "\n",
    "In order to predict latency through GNN, we need to build adjacent matrix and feature matrix of an original model.\n",
    "\n",
    "### Step 1.1: Define URL and Hardware\n",
    "\n",
    "We first give the url to download the data. And we have four types of hardware: CortexA76 CPU, Adreno 640 GPU, Adreno 630 GPU and Myriad VPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11f56feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "RAW_DATA_URL = \"https://github.com/microsoft/nn-Meter/releases/download/v1.0-data/datasets.zip\"\n",
    "\n",
    "hws = [\n",
    "    \"cortexA76cpu_tflite21\",\n",
    "    \"adreno640gpu_tflite21\",\n",
    "    \"adreno630gpu_tflite21\",\n",
    "    \"myriadvpu_openvino2019r2\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31db8cc",
   "metadata": {},
   "source": [
    "### Step 1.2: Construct our graph\n",
    "\n",
    "We now download the dataset, load latency and the corresponding model data, and build the graphs. In order to reduce the time of data loading, we also saved the processed data for convenient use next time. You can change the dgl-cu version shown below based on your CUDA version. Since our CUDA version is 11.0, we install dgl-cu110 here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6448e80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install jsonlines\n",
    "# !pip install dgl-cu110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c559aacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import jsonlines\n",
    "\n",
    "from torch.serialization import save\n",
    "\n",
    "class LatencyDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, data_dir='./dataset', train=True, device='cpu', split_ratio=0.8):\n",
    "        \"\"\"\n",
    "        Dataloader of the Latency Dataset\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_dir : string\n",
    "            Path to save the downloaded dataset\n",
    "        train: bool\n",
    "            Get the train dataset or the test dataset\n",
    "        device: string\n",
    "            The Device type of the corresponding latency\n",
    "        shuffle: bool\n",
    "            If shuffle the dataset at the begining of an epoch\n",
    "        batch_size: int\n",
    "            Batch size.\n",
    "        split_ratio: float\n",
    "            The ratio to split the train dataset and the test dataset.\n",
    "        \"\"\"\n",
    "        err_str = \"Only support device type cpu/gpu640/gpu630/vpu\"\n",
    "        assert device in ['cpu', 'gpu640', 'gpu630', 'vpu'], err_str\n",
    "        if device == 'cpu':\n",
    "            self.device = hws[0]\n",
    "        elif device == 'gpu640':\n",
    "            self.device = hws[1]\n",
    "        elif device == 'gpu630':\n",
    "            self.device = hws[2]\n",
    "        else:\n",
    "            self.device = hws[3]\n",
    "        self.data_dir = data_dir\n",
    "        self.train = train\n",
    "        self.split_ratio = split_ratio\n",
    "        self.adjs = {}\n",
    "        self.attrs = {}\n",
    "        self.nodename2id = {}\n",
    "        self.id2nodename = {}\n",
    "        self.op_types = set()\n",
    "        self.opname2id = {}\n",
    "        self.raw_data = {}\n",
    "        self.name_list = []\n",
    "        self.latencies = {}\n",
    "        self.download_data()\n",
    "        if self.train:\n",
    "            if not os.path.exists(os.path.join(self.data_dir, 'train_data_package.pkl')):\n",
    "                self.load_model_archs_and_latencies(self.data_dir)\n",
    "                self.construct_attrs()\n",
    "                self.save_data()\n",
    "            else:\n",
    "                self.load_data()\n",
    "\n",
    "        else:\n",
    "            if not os.path.exists(os.path.join(self.data_dir, 'test_data_package.pkl')):\n",
    "                self.load_model_archs_and_latencies(self.data_dir)\n",
    "                self.construct_attrs()\n",
    "                self.save_data()\n",
    "            else:\n",
    "                self.load_data()\n",
    "\n",
    "        self.name_list = list(\n",
    "            filter(lambda x: x in self.latencies, self.name_list))\n",
    "\n",
    "\n",
    "\n",
    "    def download_data(self):\n",
    "        print(\"Downloading.\")\n",
    "        if not os.path.exists(self.data_dir):\n",
    "            os.makedirs(self.data_dir, exist_ok=True)\n",
    "            os.system('wget -P %s  %s' % (self.data_dir, RAW_DATA_URL))\n",
    "            os.system('unzip %s/datasets.zip -d %s' %\n",
    "                      (self.data_dir, self.data_dir))\n",
    "\n",
    "    def load_model_archs_and_latencies(self, data_dir):\n",
    "        filelist = os.listdir(data_dir)\n",
    "        for filename in filelist:\n",
    "            if os.path.splitext(filename)[-1] != '.jsonl':\n",
    "                continue\n",
    "            self.load_model(os.path.join(data_dir, filename))\n",
    "\n",
    "    def load_model(self, fpath):\n",
    "        \"\"\"\n",
    "        Load a concrete model type.\n",
    "        \"\"\"\n",
    "        print('Loading models in ', fpath)\n",
    "        assert os.path.exists(fpath), '{} does not exists'.format(fpath)\n",
    "\n",
    "        with jsonlines.open(fpath) as reader:\n",
    "            _names = []\n",
    "            for obj in reader:\n",
    "                if obj[self.device]:\n",
    "                    # print(obj['id'])\n",
    "                    _names.append(obj['id'])\n",
    "                    self.latencies[obj['id']] = float(obj[self.device])\n",
    "\n",
    "            _names = sorted(_names)\n",
    "            split_ratio = self.split_ratio if self.train else 1-self.split_ratio\n",
    "            count = int(len(_names) * split_ratio)\n",
    "\n",
    "            if self.train:\n",
    "                _model_names = _names[:count]\n",
    "            else:\n",
    "                _model_names = _names[-1*count:]\n",
    "\n",
    "            self.name_list.extend(_model_names)\n",
    "\n",
    "        with jsonlines.open(fpath) as reader:\n",
    "            for obj in reader:\n",
    "                if obj['id'] in _model_names:\n",
    "                    model_name = obj['id']\n",
    "                    model_data = obj['graph']\n",
    "                    self.parse_model(model_name, model_data)\n",
    "                    self.raw_data[model_name] = model_data\n",
    "    \n",
    "\n",
    "    def construct_attrs(self):\n",
    "        \"\"\"\n",
    "        Construct the attributes matrix for each model.\n",
    "        Attributes tensor:\n",
    "        one-hot encoded type + input_channel , output_channel,\n",
    "        input_h, input_w + kernel_size + stride\n",
    "        \"\"\"\n",
    "        op_types_list = list(sorted(self.op_types))\n",
    "        for i, _op in enumerate(op_types_list):\n",
    "            self.opname2id[_op] = i\n",
    "        n_op_type = len(self.op_types)\n",
    "        attr_len = n_op_type + 6\n",
    "        for model_name in self.raw_data:\n",
    "            n_node = len(self.raw_data[model_name])\n",
    "            # print(\"Model: \", model_name, \" Number of Nodes: \", n_node)\n",
    "            t_attr = torch.zeros(n_node, attr_len)\n",
    "            for node in self.raw_data[model_name]:\n",
    "                node_attr = self.raw_data[model_name][node]\n",
    "                nid = self.nodename2id[model_name][node]\n",
    "                op_type = node_attr['attr']['type']\n",
    "                op_id = self.opname2id[op_type]\n",
    "                t_attr[nid][op_id] = 1\n",
    "                other_attrs = self.parse_node(model_name, node)\n",
    "                # t_attr[nid+1][-6:] = other_attrs\n",
    "                t_attr[nid][-6:] = other_attrs\n",
    "            # t_attr[0][n_op_type] = 1 # global\n",
    "            self.attrs[model_name] = t_attr\n",
    "\n",
    "    def parse_node(self, model_name, node_name):\n",
    "        \"\"\"\n",
    "        Parse the attributes of specified node\n",
    "        Get the input_c, output_c, input_h, input_w, kernel_size, stride\n",
    "        of this node. Note: filled with 0 by default if this doesn't have\n",
    "        coressponding attribute.\n",
    "        \"\"\"\n",
    "        node_data = self.raw_data[model_name][node_name]\n",
    "        t_attr = torch.zeros(6)\n",
    "        op_type = node_data['attr']['type']\n",
    "        if op_type =='Conv2D':\n",
    "            weight_shape = node_data['attr']['attr']['weight_shape']\n",
    "            kernel_size, _, in_c, out_c = weight_shape\n",
    "            stride, _= node_data['attr']['attr']['strides']\n",
    "            _, h, w, _ = node_data['attr']['output_shape'][0]\n",
    "            t_attr = torch.tensor([in_c, out_c, h, w, kernel_size, stride])\n",
    "        elif op_type == 'DepthwiseConv2dNative':\n",
    "            weight_shape = node_data['attr']['attr']['weight_shape']\n",
    "            kernel_size, _, in_c, out_c = weight_shape\n",
    "            stride, _= node_data['attr']['attr']['strides']\n",
    "            _, h, w, _ = node_data['attr']['output_shape'][0]\n",
    "            t_attr = torch.tensor([in_c, out_c, h, w, kernel_size, stride])\n",
    "        elif op_type == 'MatMul':\n",
    "            in_node = node_data['inbounds'][0]\n",
    "            in_shape = self.raw_data[model_name][in_node]['attr']['output_shape'][0]\n",
    "            in_c = in_shape[-1]\n",
    "            out_c = node_data['attr']['output_shape'][0][-1]\n",
    "            t_attr[0] = in_c\n",
    "            t_attr[1] = out_c\n",
    "        elif len(node_data['inbounds']):\n",
    "            in_node = node_data['inbounds'][0]\n",
    "            h, w, in_c, out_c = 0, 0, 0, 0\n",
    "            in_shape = self.raw_data[model_name][in_node]['attr']['output_shape'][0]\n",
    "            in_c = in_shape[-1]\n",
    "            if 'ConCat' in op_type:\n",
    "                for i in range(1, len(node_data['in_bounds'])):\n",
    "                    in_shape = self.raw_data[node_data['in_bounds']\n",
    "                                             [i]]['attr']['output_shape'][0]\n",
    "                    in_c += in_shape[-1]\n",
    "            if len(node_data['attr']['output_shape']):\n",
    "                out_shape = node_data['attr']['output_shape'][0]\n",
    "                # N, H, W, C\n",
    "                out_c = out_shape[-1]\n",
    "                if len(out_shape) == 4:\n",
    "                    h, w = out_shape[1], out_shape[2]\n",
    "            t_attr[-6:-2] = torch.tensor([in_c, out_c, h, w])\n",
    "\n",
    "        return t_attr\n",
    "\n",
    "    def parse_model(self, model_name, model_data):\n",
    "        \"\"\"\n",
    "        Parse the model data and build the adjacent matrixes\n",
    "        \"\"\"\n",
    "        n_nodes = len(model_data)\n",
    "        m_adj = torch.zeros(n_nodes, n_nodes, dtype=torch.int32)\n",
    "        id2name = {}\n",
    "        name2id = {}\n",
    "        tmp_node_id = 0\n",
    "        # build the mapping between the node name and node id\n",
    "\n",
    "        for node_name in model_data.keys():\n",
    "            id2name[tmp_node_id] = node_name\n",
    "            name2id[node_name] = tmp_node_id\n",
    "            op_type = model_data[node_name]['attr']['type']\n",
    "            self.op_types.add(op_type)\n",
    "            tmp_node_id += 1\n",
    "\n",
    "        for node_name in model_data:\n",
    "            cur_id = name2id[node_name]\n",
    "            for node in model_data[node_name]['inbounds']:\n",
    "                if node not in name2id:\n",
    "                    # weight node\n",
    "                    continue\n",
    "                in_id = name2id[node]\n",
    "                m_adj[in_id][cur_id] = 1\n",
    "            for node in model_data[node_name]['outbounds']:\n",
    "                if node not in name2id:\n",
    "                    # weight node\n",
    "                    continue\n",
    "                out_id = name2id[node]\n",
    "                m_adj[cur_id][out_id] = 1\n",
    "        \n",
    "        for idx in range(n_nodes):\n",
    "            m_adj[idx][idx] = 1\n",
    "\n",
    "        self.adjs[model_name] = m_adj\n",
    "        self.nodename2id[model_name] = name2id\n",
    "        self.id2nodename[model_name] = id2name\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        model_name = self.name_list[index]\n",
    "        return (self.adjs[model_name], self.attrs[model_name]), self.latencies[model_name]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.name_list)\n",
    "\n",
    "    def save_data(self):\n",
    "        data_package = {}\n",
    "        data_package['adjs'] = self.adjs\n",
    "        data_package['attrs'] = self.attrs\n",
    "        data_package['nodename2id'] = self.nodename2id\n",
    "        data_package['latencies'] = self.latencies\n",
    "        data_package['id2nodename'] = self.id2nodename\n",
    "        data_package['op_types'] = self.op_types\n",
    "        data_package['opname2id'] = self.opname2id\n",
    "        data_package['raw_data'] = self.raw_data\n",
    "        data_package['name_list'] = self.name_list\n",
    "        if self.train:\n",
    "            with open(os.path.join(self.data_dir, 'train_data_package.pkl'),\"wb\") as file:\n",
    "                pickle.dump(data_package, file)\n",
    "            print(\"Processing and saving train data successfully.\")\n",
    "        else:\n",
    "            with open(os.path.join(self.data_dir, 'test_data_package.pkl'),\"wb\") as file:\n",
    "                pickle.dump(data_package, file)\n",
    "            print(\"Processing and saving test data successfully.\")\n",
    "\n",
    "    def load_data(self):\n",
    "        if self.train:\n",
    "            with open(os.path.join(self.data_dir, 'train_data_package.pkl'), \"rb\") as file:\n",
    "                data_package = pickle.load(file)\n",
    "                self.adjs = data_package['adjs']\n",
    "                self.attrs = data_package['attrs']\n",
    "                self.nodename2id = data_package['nodename2id']\n",
    "                self.latencies = data_package['latencies']\n",
    "                self.id2nodename = data_package['id2nodename']\n",
    "                self.op_types = data_package['op_types']\n",
    "                self.opname2id = data_package['opname2id']\n",
    "                self.raw_data = data_package['raw_data']\n",
    "                self.name_list = data_package['name_list']\n",
    "            print(\"Loading train data successfully.\")\n",
    "        else:\n",
    "            with open(os.path.join(self.data_dir, 'test_data_package.pkl'), \"rb\") as file:\n",
    "                data_package = pickle.load(file)\n",
    "                self.adjs = data_package['adjs']\n",
    "                self.attrs = data_package['attrs']\n",
    "                self.nodename2id = data_package['nodename2id']\n",
    "                self.latencies = data_package['latencies']\n",
    "                self.id2nodename = data_package['id2nodename']\n",
    "                self.op_types = data_package['op_types']\n",
    "                self.opname2id = data_package['opname2id']\n",
    "                self.raw_data = data_package['raw_data']\n",
    "                self.name_list = data_package['name_list']\n",
    "            print(\"Loading test data successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db19376a",
   "metadata": {},
   "source": [
    "# Step 2: Build our Dataloader\n",
    "\n",
    "We build our DataLoader here with the help of the DGL library, and we will pass latency of the model as well as the graph built from the DGL library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6545ae5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import dgl\n",
    "\n",
    "MAX_NORM = torch.tensor([1]*20 + [6963, 6963, 224, 224, 11, 4])\n",
    "def default_transform(t_in):\n",
    "    return t_in/MAX_NORM\n",
    "\n",
    "class DGLDataloader(torch.utils.data.DataLoader):\n",
    "    def __init__(self, dataset, transforms=default_transform, shuffle=False, batchsize=1):\n",
    "        self.dataset = dataset\n",
    "        self.shuffle = shuffle\n",
    "        self.batchsize = batchsize\n",
    "        self.transforms = transforms # used to normalized the features\n",
    "        self.length = len(self.dataset)\n",
    "        self.indexes = list(range(self.length))\n",
    "        self.pos = 0\n",
    "        self.graphs = {}\n",
    "        self.latencies = {}\n",
    "        self.construct_graphs()\n",
    "\n",
    "    def construct_graphs(self):\n",
    "        for gid in range(self.length):\n",
    "            (adj, attrs), latency = self.dataset[gid]\n",
    "            u, v = torch.nonzero(adj, as_tuple=True)\n",
    "            # import pdb; pdb.set_trace()\n",
    "            graph = dgl.graph((u, v))\n",
    "            if self.transforms:\n",
    "                attrs = self.transforms(attrs)\n",
    "            graph.ndata['h'] = attrs\n",
    "            self.graphs[gid] = graph\n",
    "            self.latencies[gid] = latency\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.indexes)\n",
    "        self.pos = 0\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __next__(self):\n",
    "        start = self.pos\n",
    "        end = min(start + self.batchsize, self.length)\n",
    "        self.pos = end\n",
    "        if end - start <= 0:\n",
    "            raise StopIteration\n",
    "        batch_indexes = self.indexes[start:end]\n",
    "        batch_graphs = [self.graphs[i] for i in batch_indexes]\n",
    "        batch_latencies = [self.latencies[i] for i in batch_indexes]\n",
    "        return torch.tensor(batch_latencies), dgl.batch(batch_graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74b3d29",
   "metadata": {},
   "source": [
    "## Step 3: Build Model and Training\n",
    "\n",
    "In this part, we will first build our GNN model, which is constructed based on GraphSAGE, and maxpooling is selected as out pooling method. Next, we will start training after the data is loaded.\n",
    "\n",
    "### Step3.1: Build our GraphSAGE Model\n",
    "\n",
    "We built our model mainly with the help of DGL library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f17ccbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import access\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "from dgl.nn.pytorch.glob import MaxPooling\n",
    "import dgl.nn as dglnn\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "\n",
    "class GNN(Module):\n",
    "    def __init__(self, \n",
    "                num_features=0, \n",
    "                num_layers=2,\n",
    "                num_hidden=32,\n",
    "                dropout_ratio=0,\n",
    "                binary_classifier=False):\n",
    "\n",
    "        super(GNN, self).__init__()\n",
    "        self.nfeat = num_features\n",
    "        self.nlayer = num_layers\n",
    "        self.nhid = num_hidden\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.gc = nn.ModuleList([dglnn.SAGEConv(self.nfeat if i==0 else self.nhid, self.nhid, 'pool') for i in range(self.nlayer)])\n",
    "        self.bn = nn.ModuleList([nn.LayerNorm(self.nhid) for i in range(self.nlayer)])\n",
    "        self.relu = nn.ModuleList([nn.ReLU() for i in range(self.nlayer)])\n",
    "        self.pooling = MaxPooling()\n",
    "        self.fc = nn.Linear(self.nhid, 1)\n",
    "        self.fc1 = nn.Linear(self.nhid, self.nhid)\n",
    "        self.dropout = nn.ModuleList([nn.Dropout(self.dropout_ratio) for i in range(self.nlayer)])\n",
    "\n",
    "        self.binary_classifier = binary_classifier\n",
    "\n",
    "    def forward_single_model(self, g, features):\n",
    "        x = self.relu[0](self.bn[0](self.gc[0](g, features)))\n",
    "        x = self.dropout[0](x)\n",
    "        for i in range(1,self.nlayer):\n",
    "            x = self.relu[i](self.bn[i](self.gc[i](g, x)))\n",
    "            x = self.dropout[i](x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        x = self.forward_single_model(g, features)\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = x\n",
    "            x = self.pooling(g, x)\n",
    "            x = self.fc1(x)\n",
    "            return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7302fb24",
   "metadata": {},
   "source": [
    "### Step 3.2: Loading Data.\n",
    "\n",
    "Next, we will finish loading the data and learn about the size of the Training and Testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d84f52ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Training Set.\n",
      "Downloading.\n",
      "Loading train data successfully.\n",
      "Processing Testing Set.\n",
      "Downloading.\n",
      "Loading test data successfully.\n",
      "Train Dataset Size: 22324\n",
      "Testing Dataset Size: 5571\n",
      "Attribute tensor shape: 26\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing Training Set.\")\n",
    "train_set = LatencyDataset('./dataset', train=True, device='cpu') \n",
    "print(\"Processing Testing Set.\")\n",
    "test_set = LatencyDataset('./dataset', train=False, device='cpu')\n",
    "\n",
    "train_loader = DGLDataloader(train_set, batchsize=1 , shuffle=True)\n",
    "test_loader = DGLDataloader(test_set, batchsize=1, shuffle=False)\n",
    "print('Train Dataset Size:', len(train_set))\n",
    "print('Testing Dataset Size:', len(test_set))\n",
    "print('Attribute tensor shape:', next(train_loader)[1].ndata['h'].size(1))\n",
    "ATTR_COUNT = next(train_loader)[1].ndata['h'].size(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e502503",
   "metadata": {},
   "source": [
    "### Step3.3: Run and Test\n",
    "\n",
    "We can run the model and evaluate it now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f229742a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Training accuracy within 10%:  22.41981723705429  %.\n",
      "Learning Rate: [0.00039753766811902755]\n",
      "Loss: tensor(130.2296, grad_fn=<DivBackward0>)\n",
      "Epoch 1\n",
      "Training accuracy within 10%:  29.667622289912206  %.\n",
      "Learning Rate: [0.00039021130325903074]\n",
      "Loss: tensor(97.9046, grad_fn=<DivBackward0>)\n",
      "Epoch 2\n",
      "Training accuracy within 10%:  30.984590575165743  %.\n",
      "Learning Rate: [0.0003782013048376736]\n",
      "Loss: tensor(88.9308, grad_fn=<DivBackward0>)\n",
      "Epoch 3\n",
      "Training accuracy within 10%:  32.516574090664754  %.\n",
      "Learning Rate: [0.0003618033988749895]\n",
      "Loss: tensor(82.0981, grad_fn=<DivBackward0>)\n",
      "Epoch 4\n",
      "Training accuracy within 10%:  34.74735710446157  %.\n",
      "Learning Rate: [0.0003414213562373095]\n",
      "Loss: tensor(78.0732, grad_fn=<DivBackward0>)\n",
      "Epoch 5\n",
      "Training accuracy within 10%:  36.34205339544884  %.\n",
      "Learning Rate: [0.00031755705045849464]\n",
      "Loss: tensor(72.6763, grad_fn=<DivBackward0>)\n",
      "Epoch 6\n",
      "Training accuracy within 10%:  37.94122917039957  %.\n",
      "Learning Rate: [0.00029079809994790937]\n",
      "Loss: tensor(69.6124, grad_fn=<DivBackward0>)\n",
      "Epoch 7\n",
      "Training accuracy within 10%:  39.58519978498477  %.\n",
      "Learning Rate: [0.00026180339887498953]\n",
      "Loss: tensor(64.9966, grad_fn=<DivBackward0>)\n",
      "Epoch 8\n",
      "Training accuracy within 10%:  40.69163232395628  %.\n",
      "Learning Rate: [0.00023128689300804623]\n",
      "Loss: tensor(63.5987, grad_fn=<DivBackward0>)\n",
      "Epoch 9\n",
      "Training accuracy within 10%:  42.95825120946067  %.\n",
      "Learning Rate: [0.00020000000000000004]\n",
      "Loss: tensor(59.1240, grad_fn=<DivBackward0>)\n",
      "Epoch 10\n",
      "Training accuracy within 10%:  44.933703637340976  %.\n",
      "Learning Rate: [0.00016871310699195392]\n",
      "Loss: tensor(53.9162, grad_fn=<DivBackward0>)\n",
      "Epoch 11\n",
      "Training accuracy within 10%:  47.22719942662606  %.\n",
      "Learning Rate: [0.00013819660112501057]\n",
      "Loss: tensor(51.1719, grad_fn=<DivBackward0>)\n",
      "Epoch 12\n",
      "Training accuracy within 10%:  49.21161082243326  %.\n",
      "Learning Rate: [0.00010920190005209068]\n",
      "Loss: tensor(48.0223, grad_fn=<DivBackward0>)\n",
      "Epoch 13\n",
      "Training accuracy within 10%:  52.18598817416233  %.\n",
      "Learning Rate: [8.244294954150542e-05]\n",
      "Loss: tensor(44.2510, grad_fn=<DivBackward0>)\n",
      "Epoch 14\n",
      "Training accuracy within 10%:  54.64074538613152  %.\n",
      "Learning Rate: [5.857864376269052e-05]\n",
      "Loss: tensor(40.9639, grad_fn=<DivBackward0>)\n",
      "Epoch 15\n",
      "Training accuracy within 10%:  57.15821537358896  %.\n",
      "Learning Rate: [3.819660112501054e-05]\n",
      "Loss: tensor(37.9240, grad_fn=<DivBackward0>)\n",
      "Epoch 16\n",
      "Training accuracy within 10%:  59.18294212506719  %.\n",
      "Learning Rate: [2.1798695162326446e-05]\n",
      "Loss: tensor(34.8407, grad_fn=<DivBackward0>)\n",
      "Epoch 17\n",
      "Training accuracy within 10%:  62.24690915606522  %.\n",
      "Learning Rate: [9.788696740969295e-06]\n",
      "Loss: tensor(32.2578, grad_fn=<DivBackward0>)\n",
      "Epoch 18\n",
      "Training accuracy within 10%:  64.15964880845728  %.\n",
      "Learning Rate: [2.4623318809724685e-06]\n",
      "Loss: tensor(30.8232, grad_fn=<DivBackward0>)\n",
      "Epoch 19\n",
      "Training accuracy within 10%:  64.31643074717792  %.\n",
      "Learning Rate: [0.0]\n",
      "Loss: tensor(30.0199, grad_fn=<DivBackward0>)\n",
      "Testing accuracy within 10%:  59.68407826243044  %.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Using CUDA.\")\n",
    "# device = \"cpu\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "load_model = False\n",
    "if load_model:\n",
    "    model = GNN(ATTR_COUNT, 3, 400, 0.1).to(device)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=4e-4)\n",
    "    checkpoint = torch.load('LatencyGNN.pt')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    opt.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    # EPOCHS = checkpoint['epoch']\n",
    "    EPOCHS = 0\n",
    "    loss_func = checkpoint['loss']\n",
    "else:\n",
    "    model = GNN(ATTR_COUNT, 3, 400, 0.1).to(device)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=4e-4)\n",
    "    EPOCHS=20\n",
    "    loss_func = nn.L1Loss()\n",
    "\n",
    "lr_scheduler = CosineAnnealingLR(opt, T_max=EPOCHS)\n",
    "loss_sum = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    train_length = len(train_set)\n",
    "    tran_acc_ten = 0\n",
    "    print('Epoch %d' % epoch)\n",
    "    loss_sum = 0 \n",
    "    # latency, graph, types, flops\n",
    "    for batched_l, batched_g in train_loader:\n",
    "        opt.zero_grad()\n",
    "        batched_l = batched_l.to(device).float()\n",
    "        batched_g = batched_g.to(device)\n",
    "        batched_f = batched_g.ndata['h'].float()\n",
    "        logits = model(batched_g, batched_f)\n",
    "        for i in range(len(batched_l)):\n",
    "            pred_latency = logits[i].item()\n",
    "            prec_latency = batched_l[i].item()\n",
    "            if (pred_latency >= 0.9 * prec_latency) and (pred_latency <= 1.1 * prec_latency):\n",
    "                tran_acc_ten += 1\n",
    "        # print(\"true latency: \", batched_l)\n",
    "        # print(\"Predict latency: \", logits)\n",
    "        batched_l = torch.reshape(batched_l, (-1 ,1))\n",
    "        loss = loss_func(logits, batched_l)\n",
    "        loss_sum += loss\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    lr_scheduler.step()\n",
    "    print(\"Training accuracy within 10%: \", tran_acc_ten / train_length * 100, \" %.\")\n",
    "    print('Learning Rate:', lr_scheduler.get_last_lr())\n",
    "    print('Loss:', loss_sum / train_length)\n",
    "\n",
    "torch.save({\n",
    "    'epoch': EPOCHS,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': opt.state_dict(),\n",
    "    'loss': loss_func,\n",
    "}, 'LatencyGNN.pt')\n",
    "\n",
    "\n",
    "count = 0\n",
    "with torch.no_grad():\n",
    "    test_length = len(test_set)\n",
    "    test_acc_ten = 0\n",
    "    for batched_l, batched_g in test_loader:\n",
    "        batched_l = batched_l.to(device).float()\n",
    "        batched_g = batched_g.to(device)\n",
    "        batched_f = batched_g.ndata['h'].float()\n",
    "        result = model(batched_g, batched_f)\n",
    "        if (result.item() >= 0.9 * batched_l.item()) and (result.item() <= 1.1 * batched_l.item()):\n",
    "            test_acc_ten += 1\n",
    "        acc = (abs(result.item() - batched_l.item()) / batched_l.item()) * 100\n",
    "        count += 1\n",
    "    print(\"Testing accuracy within 10%: \", test_acc_ten / test_length * 100, \" %.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
